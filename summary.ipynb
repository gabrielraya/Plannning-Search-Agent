{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "<div style=\"text-align: justify\">   \n",
    "Let's summarize what we have seen so far. We have went all the way since the definition of AI, its historical events until the concepts of logical representations. Therefore, this section provides a brief review of all the concepts previously seen in order to understand why we need this Planning Domain Definition Language to scale up to problems that could not be handled by those earlier approaches.\n",
    "</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">   \n",
    "<strong>Chapter 1</strong>, <em>Artificial Intelligence</em>, defined the concept of Intelligence, AI, the history of AI, and state of the art applications. It identified the concept of <strong>rational agents</strong> as central to our approach to artificial intelligence.\n",
    "</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 2</strong>, <em>Intelligent Agents</em>, made this notion of a rational agent more concrete. It introduced the idea of intelligent agents to be defined by PEAS (peformance, environment, actuators and sensors), giving a small set of design principles for building successful agents--systems that can reasonably called <strong>intelligent</strong>. We began by examining agents, environments, and the coupling between them. The observation that some agents behave better than others led naturally to the idea of a rational agent--one that behaves as well as possible. How well an agent can behave depends on the nature of the envrionment; some environments are more difficult than others. It gave a crude categorization of environments and show how properties of an envrionment influece the design of suitable agents for that environment. (fully vs partially observable, deterministic vs stochastic, static vs dynamic, discrete vs continous, known vs unknown, episodic vs sequential). Finally Chapter 2 discussed simplest agents like <em>reflex agents</em>, which base their actions on a direct mapping from states to actions. Such agents cannot operate well in environments for which this mapping would be too large to store and would take too long to learn (such the <strong>West Agent</strong> in the Pac Man Problem), goal agents, on the other hand, consider furture actions (LOOK AHEAD APPROACH) and the desirability of their outcomes (examples like the routing problem and the Pac Man Project).\n",
    "</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 3</strong>, <em>Problem-solving</em>, described one kind o goal-based agent called a <strong>problem-solving agent</strong>. Problem-solving agents use <strong>atomic</strong> representations, that is, states of the world are considered as wholes, with no internal structure visible to the problem-solving algorithms. Goal-based agents that use more advance <strong>factored</strong> or <strong>structured</strong> representations are usually called <strong>planning agents</strong> and are disccussed in Chapter 7 an 10. The discussion of problem solving began with precise definitions of <strong>problems</strong> and their <strong>solutions</strong>. Then it described several general-purpose search algorithms that  can be used to solve these problems. We saw several <strong>uninformed</strong> search algorithms--algorithms that are given no information about the problem other than its definition. Although some of these algorithms can solve any solvable problem, none of them can do so efficiently. <strong>Informed</strong> searcha algorithms, on the other hand, can do quite well given some guidance on where to look for solutions (heuristic function). Therefore, Chapter 2 limits itself to the simples kind of task environment, for which the solution to a problem is always a <em>fixed sequence</m> of actions. The more general case--where the agent's future actiosn may vary depending on future percepts--is handled in Chapter 4. \n",
    "</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 4</strong>, <em>Beyond Classical Search</em>. Chapter 3 addressed a single category of problems: observable, deterministic, known environments where the solution is a sequence of action. In this chapter, we look at what happens when these assumptions are relaxed. It began with a fairly simple case: covered algorithms that perform purely <strong>local search</strong> in the state space, evaluating and modifying one or more current states rather than systematically exploring paths from an initial state. These algorithms are suitable for problems in which all that matters is the solution state, not the path costo to reach it. The family of local search algorithms includes methods inspired by statistical physics (<strong>simulated annealing</strong>) and evolutionary biology (<strong>genetic algorithm</strong>). Then, it examined what happens when we relax the assumptions of determinism and observability. The key idea is that if an agent cannot predict exactly what percept it will receive, then it will need to consider what to do under each <strong>contingency</strong> that its percpets may reveal. With partial observability, the agent will also need to keep track of the states it might be in. Finally it investigated <strong>online search</strong>, in which the agent is faced with a state space that is initiallly unknown and must be explored. \n",
    "</div><br>\n",
    " \n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 5</strong>, <em>Adversarial Search</em>. Chapter 2 introduced <strong>mutiagent envrionments></strong>, in which each agent needs to consider the actions of other agents and how they affect its own welfare. The unpredictability of these other agents can introduce <strong>contingencies</strong> into the agent's problem-solving process, as discussed in Chapter 4. This Chapter covered <strong>competitive</strong> environments, in which the agents' goals are in conflict, giving rise to <strong>adversarial search</strong> problems--often known as <strong>games</strong>. In AI, the most common games are of a rather specialized kind--what game theorists call deterministic, turn-taking, two-player, <strong>zero-sum games</strong> of <strong>perfect information</strong>(such as chess). In our terminology, this means deterministic, fully observable environments in which two agents act alternately and in which the utility values at the end of the game are always equal and opposite. For example, if one player wins a game of chess, the other player necessarily loses. It is this opposition betweenteh agents's utility fucntions that makes the situation adversarial.<br> Games have engaged the intellectual faculties of humans--sometimes to an alarming degree-- for as long as civilization has existed. For AI researches, the abstract nature of games makes them an appelaing subject of study.The state of a game is easy to represent, and agents are usually restricted to a small number of actions whose outcomes are defined by precise rules. Physical games, such as croquet and ice hockey, have much more complicated\n",
    "descriptions, a much larger range of possible actions, and rather imprecise rules defining\n",
    "the legality of actions. With the exception of robot soccer, these physical games have not\n",
    "attracted much interest in the AI community.\n",
    "</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 6</strong>, <em> Constraint Satisfaction Problems</em>. Chapter 3 and 4 explored the idea that problems can be solved by searching in a space of <strong>states</strong>. These states can be evaluated by domain-specific heuristics and tested to see whether they are goal states. From the point of view of the search algorithm, however, each state is atomic, or indivisible--a black box with no internal structure. This chapter the, described a way to solve a wide variety of problems more efficiently. We use a <strong>factored representation</strong> for each state> a set of variables, each of which has a value. A problem is solved when each variable has a value that satisfies all the constraints on the variable. A problem described this way is called a <strong>constraint satisfaction problem</strong>, or CSP.<br>\n",
    "CSP search algorithms take advantage of the structure of states and use <em>general-purspose</em> rather than <em>problem-specific</em> heuristics to enable the solution of complex problems. The main idea is to eliminate large portions of the search space all at once by identifying variable/value combinations that violate the constraints.\n",
    "</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 7</strong>, <em> Logical Agents</em>, talks about reasoning, representation and knowledge-based agents. It started describing this: Humans, it seems, know things; and what they know helps them to do things. These are not empty statements. They make strong claims about how the intelligence of humans is achieved--not by purely reflex mechanisms but by processes of <strong>reasoning</strong> that operate on internal <strong>representations</strong> of knowledge. In AI, this approach to intelligence is embodied in <strong>knowledge-based agents</strong>.\n",
    "The problem-solving agents of Chapter 3 and 4 know things, but only in a very limited, inflexible sense. For example, the transition model for the 8-puzzle—knowledge of what the actions do—is hidden inside the domain-specific code of the RESULT function. It can be used to predict the outcome of actions but not to deduce that two tiles cannot occupy the same space or that states with odd parity cannot be reached from states with even parity. The atomic representations used by problem-solving agents are also very limiting. In a partially observable environment, an agent’s only choice for representing what it knows about the current state is to list all possible concrete states—a hopeless prospect in large environments. Chapter 6 introduced the idea of representing states as assignments of values to variables; this is a step in the right direction, enabling some parts of the agent to work in a domain-independent way and allowing for more efficient algorithms. In this chapter and\n",
    "those that follow, we take this step to its logical conclusion, so to speak—we develop logic as a general class of representations to support knowledge-based agents. Such agents can combine and recombine information to suit myriad purposes. Often, this process can be quite far removed from the needs of the moment—as when a mathematician proves a theorem or an astronomer calculates the earth’s life expectancy. Knowledge-based agents can accept new tasks in the form of explicitly described goals; they can achieve competence quickly by being told or learning new knowledge about the environment; and they can adapt to changes in the environment by updating the relevant knowledge.<br>\n",
    "It begins in Section 7.1 with the overall agent design. Section 7.2 introduces a simple new environment, the wumpus world, and illustrates the operation of a knowledge-based agent without going into any technical detail. Then we explain the general principles of <strong>logic</strong> in Section 7.3 and the specifics of <strong>propositional logic</strong> in Section 7.4. While less expressive\n",
    "than <strong>first-order logic</strong> (Chapter 8), propositional logic illustrates all the basic concepts of logic; it also comes with well-developed inference technologies, which we describe in sections 7.5 and 7.6. Finally, Section 7.7 combines the concept of knowledge-based agents with\n",
    "the technology of propositional logic to build some simple agents for the wumpus world.\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 8</strong>, <em>First-Order Logic</em>. In Chapter 7, we saw how a knwoledge-based agent could represent the world in which it operates and deduce what actions to take (inference). We used propositional logic as our representation language because is sufficed to illustrate the basic concepts of logic and knowledge-based agents. Unfortunately, propsitional logic is too puny a language to represent knowledge of complex environments in a concise way. In this Chapter, we examined <strong>first-order logic</strong>, which is sufficiently expressive to represent a good deal of our commonsense knowledge. It also either subsumes or forms the foundation of many other representation languages and has been studied intensively for many decades. \n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 9</strong>,<em>Inference in First-Order Logic</em>. Chapter 7 showed how sound and complete inference can be achieved for propositional logic. In this chapter, we extend those results to obtain algorithms tha can answer any answerable question stated in first-order logic. Section 9.1 introduces inference rules for quantifiers and shows how to reduce first-order inference to propositional inference, albeit at potentially great expense. Section 9.2 describes the idea of unification, showing how it can be used to construct inference rules that work directly with first-order sentences. We then discuss three major families of first-order inference algorithms. Forward chaining and its applications to deductive databases and production systems are covered in Section 9.3; backward chaining and logic programming systems are developed in Section 9.4. Forward and backward chaining can be very efficient, but are applicable only to knowledge bases that can\n",
    "be expressed as sets of Horn clauses. General first-order sentences require resolution-based\n",
    "theorem proving, which is described in Section 9.5. \n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<strong>Chapter 10</strong>,<em>Clasical Planning</em>. We have defined AI as the study of rational action, which means that <strong>planning</strong>-devising a plan of action to achieve one's goals-is a critical part of AI. We have seen two examples of planning agents so far: the search-based problem-solving agent of Chapter 3 and the hybrid logical agent of Chapter 7. In this chapter we introduce a representation for planning problems that scales up to problems that could not be handled by those earlier approaches.\n",
    "Section 10.1 developes an expressive yet carefully contrained language for representing planning problems. Section 10.2 shows how forward and backward search algorithms can take advantage of this representation, primarily through accurate heuristics that can be derived automatically from the structure of the representation. (This is analogous to the way in which effective domain-independent heuristics were constructed for constraint satisfaction problems in Chapter 6.) Section 10.3 shows how a data structure called the planning graph can make the search for a plan more efficient. We then describe a few of the other approaches to planning, and conclude by comparing the various approaches.\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
